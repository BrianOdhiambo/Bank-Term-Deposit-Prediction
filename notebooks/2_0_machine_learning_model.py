# -*- coding: utf-8 -*-
"""2.0-machine-learning-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VPxApY8hLcN5Oz8ZNaUhqvUjzPK7sX6M

# Building Predictive Models
"""

import pandas as pd
import os 
import numpy as np

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier 
# performance metrics
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, precision_recall_curve

# Dummy Classifier
from sklearn.dummy import DummyClassifier

# Logistic Regression
from sklearn.linear_model import LogisticRegression

# XGBoost
from xgboost import XGBClassifier

# multilayer perceptron
from sklearn.neural_network import MLPClassifier

# model explanation
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import auc

from sklearn.ensemble import IsolationForest

import matplotlib.pyplot as plt

# for warning ignore
import warnings
warnings.filterwarnings("ignore")

"""## Import Data"""

# set the path of the processed data
processed_data_path = os.path.join(os.path.pardir, 'data', 'processed')
train_file_path = os.path.join(processed_data_path, 'train.csv')
test_file_path = os.path.join(processed_data_path, 'test2.csv')

X_train = pd.read_csv(train_file_path)
y_train = pd.read_csv(test_file_path)

#X = X_train.loc[:,:].as_matrix().astype('float')
#y = y_train['subscription'].ravel()

X = X_train
y = y_train

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=0)

"""## Baseline Model"""

# create model 
model_dummy = DummyClassifier(strategy='most_frequent', random_state=0)
# train model
model_dummy.fit(X_train, y_train)

y_pred_dummy = model_dummy.predict(X_test)
print("Accuracy of baseline model classifier on test set: {:.2f}".format(model_dummy.score(X_test, y_test)))

"""# ROC Curve

The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 â€“ FPR). Classifiers that give curves closer to the top-left corner indicate a better performance
"""

logit_roc_auc = roc_auc_score(y_test, model_dummy.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, model_dummy.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Baseline Model (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
#plt.savefig('Log_ROC')
plt.show()

figure_path = os.path.join(os.path.pardir, 'reports','figures')

def model_classifier(model, X, y, cv):
    """
    Creates folds manually, perform 
    Returns an array of validation (recall) scores
    """
    scores = []
    
    
    for train_index,test_index in cv.split(X,y):
        X_train,X_test = X.loc[train_index],X.loc[test_index]
        y_train,y_test = y.loc[train_index],y.loc[test_index]

        # Fit the model on the training data
        model_obj = model.fit(X_train, y_train)
        y_pred = model_obj.predict(X_test)
        y_pred_prob = model_obj.predict_proba(X_test)[:,1]
        # Score the model on the validation data
        score = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred)
        conf_matrix = confusion_matrix(y_test, y_pred)
        
        scores.append(score)
        mean_score = np.array(scores).mean()    

    print('Accuracy scores of the model: {:.2f}'.format(mean_score))
    print('\n Classification report of the model')
    print('--------------------------------------')
    print(report)
    
    print('\n Confusion Matrix of the model')
    print('--------------------------------------')
    print(conf_matrix)
    
    print("\n ROC Curve")
    
    logit_roc_auc = roc_auc_score(y_test, y_pred)
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
    plt.figure()
    val_model = input("Enter your model name: ")
    plt.plot(fpr, tpr, label= val_model + ' (area = %0.2f)' % logit_roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    my_fig = val_model + '.png'
    plt.savefig(os.path.join(figure_path, my_fig))
    plt.show()

"""# Logistic Regression"""

lr = LogisticRegression()
kf = KFold(n_splits = 10, shuffle = True, random_state = 4)
skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 4)

"""## KFold"""

model_classifier(lr, X, y, kf)

"""## Stratified KFold"""

model_classifier(lr, X, y, skf)

"""## Logistic Regression Hyperparameter Tuning"""

lr_hyp = LogisticRegression()

# regularization penalty space
penalty = ['l1','l2']
solver = ['liblinear', 'saga']

# regularization hyperparameter space
#C = np.logspace(0, 4, 10)
C = np.logspace(0, 4, num=10)

# hyperparameter options
param_grid = dict(C=C, penalty=penalty, solver=solver)

log_reg_cv = RandomizedSearchCV(lr_hyp, param_grid)

model_classifier(log_reg_cv, X, y, kf)

"""### Stratified KFold"""

model_classifier(log_reg_cv, X, y, skf)

"""# XGBoost"""

xgb = XGBClassifier()

"""### XGBoost KFold"""

model_classifier(xgb, X, y, kf)

"""### XGBoost : Stratified KFold"""

model_classifier(xgb, X, y, skf)

"""### Hyperparameter tuning XGBoost"""

def timer(start_time=None):
    if not start_time:
        start_time = datetime.now()
        return start_time
    elif start_time:
        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)
        tmin, tsec = divmod(temp_sec, 60)
        print('\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))

# A parameter grid for XGBoost
from datetime import datetime
params = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }

xgb_hyp = XGBClassifier()

xgb_random_search = RandomizedSearchCV(xgb_hyp, param_distributions=params, n_iter=5, scoring='roc_auc', n_jobs=-1, cv=10, verbose=3)


start_time = timer(None) # timing starts from this point for "start_time" variable
xgb_random_search.fit(X, y)
timer(start_time)

xgb_random_search.best_estimator_

xgb_random_search.best_params_

xgb_tuned = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1.0, gamma=0.5, gpu_id=-1,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.300000012, max_delta_step=0, max_depth=4,
              min_child_weight=1, missing=None, monotone_constraints='()',
              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1.0,
              tree_method='exact', validate_parameters=1, verbosity=None)
model_classifier(xgb_tuned,X,y,skf)

"""## MLP Classifier"""

mlp = MLPClassifier()

"""### KFold"""

model_classifier(mlp, X, y, kf)

"""### Stratified KFold"""

model_classifier(mlp, X, y, skf)

"""## Hyperparameter Tuning : MLP"""

parameter_space = {
    'hidden_layer_sizes': [(10,30,10),(20,)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant','adaptive'],
}
mlp_hyp = MLPClassifier()
randomized_mlp = RandomizedSearchCV(mlp_hyp, parameter_space, n_jobs=-1, cv=10)
start_time = timer(None) # timing starts from this point for "start_time" variable
randomized_mlp.fit(X, y)
timer(start_time)

randomized_mlp.best_estimator_

randomized_mlp.best_params_

tuned_mlp = MLPClassifier(hidden_layer_sizes=(20,), learning_rate='adaptive', solver='sgd')

model_classifier(tuned_mlp, X, y, skf)

